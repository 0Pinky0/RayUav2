algo_name: SAC
ckpt: null
environment:
  env: Ray/Pong-v5
#  env: Ray/SpaceInvaders-v5
  is_atari: True
training:
  model:
    uses_new_env_runners: True
  lr: 1.0e-4
  gamma: 0.997
  train_batch_size: 512
  # Algo Specific
  target_network_update_freq: 400
  num_steps_sampled_before_learning_starts: 4_000
  dueling: True
  double_q: True
  n_step: 1
  epsilon: [ [ 0, 0.6 ], [ 150_000, 0.01 ] ]
  td_error_loss_fn: mse
  replay_buffer_config:
    type: PrioritizedEpisodeReplayBuffer
    capacity: 100_000
    alpha: 0.9
    beta: 0.6
stop:
#  env_runners/episode_return_mean: 20.0
  training_iteration: 200
  num_env_steps_sampled_lifetime: 5000000